<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face-Controlled Spline</title>
  <script type="module">
    import { Application } from 'https://unpkg.com/@splinetool/runtime@1.9.82/build/runtime.js';

    window.onload = async () => {
      // Initialize Spline
      const canvas = document.getElementById('spline-canvas');
      const app = new Application(canvas);
      try {
        await app.load('https://prod.spline.design/abU-ltf0N5uCFYyP/scene.splinecode');
        console.log("Spline scene loaded successfully");
        const lep = app.findObjectByName('lep');
        console.log("Lep object found:", lep ? "Yes" : "No");
        console.log("All objects in scene:", app.getAllObjects().map(obj => obj.name));
      } catch (err) {
        console.error("Failed to load Spline scene:", err);
        return;
      }

      // Request camera access
      try {
        const stream = await navigator.mediaDevices.getUserMedia({
          video: { 
            width: { ideal: 640 }, 
            height: { ideal: 480 }, 
            aspectRatio: { ideal: 480/640 }, 
            frameRate: { ideal: 15 },
            facingMode: "user" // Ensure front-facing camera
          }
        });
        document.getElementById('video').srcObject = stream;
        console.log("Camera access granted");
      } catch (err) {
        console.error("Camera access failed:", err);
        alert("Camera access required for face tracking.");
        return;
      }

      // Initialize face tracking
      const faceMesh = new window.FaceMesh({
        locateFile: file => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
      });
      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: false,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      });

      let isLooking = false;
      let stateTimeout = null;
      const debugOverlay = document.getElementById('debug-overlay');

      faceMesh.onResults(results => {
        if (!results.multiFaceLandmarks[0]) {
          console.log("No face detected");
          debugOverlay.style.backgroundColor = 'rgba(255, 0, 0, 0.5)'; // Red when no face
          return;
        }
        console.log("Face detected");

        const lep = app.findObjectByName('lep');
        if (!lep) {
          console.log("Lep object not found");
          return;
        }

        // Log raw landmarks
        const landmarks = results.multiFaceLandmarks[0];
        const nose = landmarks[1]; // Nose tip
        const leftEye = landmarks[33]; // Left eye outer
        const rightEye = landmarks[263]; // Right eye outer
        console.log("Raw landmarks:", {
          nose: { x: nose.x.toFixed(2), y: nose.y.toFixed(2), z: nose.z.toFixed(2) },
          leftEye: { x: leftEye.x.toFixed(2), y: leftEye.y.toFixed(2), z: leftEye.z.toFixed(2) },
          rightEye: { x: rightEye.x.toFixed(2), y: rightEye.y.toFixed(2), z: rightEye.z.toFixed(2) }
        });

        // Estimate head orientation (yaw/pitch)
        const yaw = Math.atan2(rightEye.y - leftEye.y, rightEye.x - leftEye.x) * (180 / Math.PI);
        const pitch = Math.atan2(nose.z - (leftEye.z + rightEye.z) / 2, nose.y - (leftEye.y + rightEye.y) / 2) * (180 / Math.PI);
        console.log(`Yaw: ${yaw.toFixed(2)}°, Pitch: ${pitch.toFixed(2)}°`);

        // Check if looking at screen (±15° for wider range)
        const lookingAtScreen = Math.abs(yaw) < 15 && Math.abs(pitch) < 15;
        console.log("Looking at screen:", lookingAtScreen);
        debugOverlay.style.backgroundColor = lookingAtScreen ? 'rgba(0, 255, 0, 0.5)' : 'rgba(255, 0, 0, 0.5)';

        // Update Spline state only on change
        if (lookingAtScreen !== isLooking) {
          isLooking = lookingAtScreen;
          clearTimeout(stateTimeout);
          console.log("State change triggered:", isLooking ? "Looking" : "Not looking");

          if (isLooking) {
            // Trigger state sequence: Base State -> State -> State 2
            lep.setState('Base State');
            console.log("Set state: Base State");
            stateTimeout = setTimeout(() => {
              lep.setState('State');
              console.log("Set state: State");
              stateTimeout = setTimeout(() => {
                lep.setState('State 2');
                console.log("Set state: State 2");
              }, 200);
            }, 200);
          } else {
            // Revert to Base State
            lep.setState('Base State');
            console.log("Set state: Base State (revert)");
          }
        }
      });

      // Start camera
      const video = document.getElementById('video');
      const camera = new window.Camera(video, {
        onFrame: async () => await faceMesh.send({ image: video }),
        width: 640,
        height: 480
      });
      camera.start().then(() => console.log("Camera started")).catch(err => console.error("Camera start failed:", err));
    };
  </script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <style>
    body { margin: 0; overflow: hidden; display: flex; justify-content: center; align-items: center; height: 100vh; }
    #spline-canvas { width: 100%; height: 100%; }
    #video { display: none; transform: rotate(-90deg); width: 480px; height: 640px; }
    #debug-overlay { 
      position: absolute; 
      top: 10px; 
      left: 10px; 
      width: 50px; 
      height: 50px; 
      pointer-events: none; 
      z-index: 10; 
    }
  </style>
</head>
<body>
  <canvas id="spline-canvas"></canvas>
  <video id="video" autoplay playsinline></video>
  <div id="debug-overlay"></div>
</body>
</html>
